{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUlonwBz4ovS"
   },
   "source": [
    "# Image Classification with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAO6J-v64xdZ"
   },
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WQwp_eTmajE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision #has various utils functions, including loading datasets\n",
    "import torchvision.transforms as transforms #common image transformations\n",
    "\n",
    "import torch.nn as nn #creating neural network\n",
    "import torch.nn.functional as F #functional api for layers,...\n",
    "import torch.optim as optim #for optimization algorithm\n",
    "\n",
    "import matplotlib.pyplot as plt #for visualization\n",
    "import numpy as np #for basic array operations\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4sGSzsd5P6k"
   },
   "source": [
    "## 1. Load Data - CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform data to tensor and normalize with mean=0.5 and standard deviation=0.5 for each channel\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#how many samples per batch to load\n",
    "batch_size = 4\n",
    "\n",
    "#load training set and apply transform\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "21de6de5723647b2b1c511f9c523d80b",
      "aab2327d02024e1fbc22178eda86f049",
      "1132508146f543888f3358fb4b33bb52",
      "f3a19f4144e54caaaf51ced4046a3165",
      "b7a93f3117914831ac1a929c622ae529",
      "ee63ff85590d4f3b9ef1511daecb53eb",
      "be84f0e6beb34398b75184f26c30bac8",
      "03953809ceab4a368c3e98be687049e1",
      "0b70957757a7404bb0949fc2518f4eaf",
      "5a1caefed15a4f83b0728604c7ed8c02",
      "de2e53173a5543cf9b74e03ec58ca279"
     ]
    },
    "id": "JZbpicHTmkJv",
    "outputId": "78b079ae-5ba5-4c79-ab89-7cb0577e41d5"
   },
   "outputs": [],
   "source": [
    "#load test set and apply transform\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "#CIFAR10 has the following 10 classes:\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGvcAJog86G1",
    "outputId": "f45609fc-28a7-40be-f4fc-0d4b56521557"
   },
   "outputs": [],
   "source": [
    "print(\"Training set: \",trainset.data.shape)\n",
    "print(\"Test set: \", testset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-CaLKK88asM"
   },
   "source": [
    "#### Plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "id": "4sFeeW_Nm4Tb",
    "outputId": "484a1b04-bff3-48a8-8795-373b6f95728a"
   },
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     #because images were normalized when loaded, for visualization purposes we unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) #reorder the channels \n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L28UWfy9JYx"
   },
   "source": [
    "## 2. Create and Train a Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRvATWztF710",
    "outputId": "2d79875d-f90d-424c-cb02-678278a3a292"
   },
   "outputs": [],
   "source": [
    "#Set up GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q6KJfam9Skr"
   },
   "source": [
    "#### Create a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZUMfNiSDsxb"
   },
   "source": [
    "Convolution and pooling are fundamental operations for building CNN models. There are a number of parameters and if their definitions are not clear, it could lead to great confusion.\n",
    "- Parameters for convolution (pooling) layers\n",
    "  - **stride:** how many \"steps\" that the filter makes for each advance\n",
    "  - **kernel size**: how large is the kernel (filter) is\n",
    "  - **number of filters (channels):** designates the \"depth\" of the data. Most image inputs have three filters (RGB)\n",
    "  - **padding:** how to pad the input sample with zero in the border\n",
    "- How to calculate output size of convolution/pooling operation\n",
    "  <br> \n",
    "*(W - F + 2P)/S + 1* <br>\n",
    "  - *W*: input size\n",
    "  - *F*: kernel size\n",
    "  - *P*: padding \n",
    "  - *S*: stride\n",
    "\n",
    "\n",
    "> The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel. - [Source](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" style=\"max-width:400px;\">\n",
    "\n",
    "The Max Pooling 2D   \n",
    "![alt text](https://user-images.githubusercontent.com/22738317/34081046-c3a97518-e347-11e7-98fe-929f602ee857.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgPehE-vf8He"
   },
   "source": [
    "Let's test the pooling operations on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FF4xyx4dR3V",
    "outputId": "20068f96-61c9-4593-df6f-16e09a7ad37e"
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([[[0,0,1],\n",
    "                   [1,2,3],\n",
    "                   [7,5,3],\n",
    "                   [5,3,6]],\n",
    "                  \n",
    "                  [[9,11,10],\n",
    "                   [10,11,12],\n",
    "                   [31,55,32],\n",
    "                   [17,29,32]]], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# t = torch.tensor([[[0,0,1],\n",
    "#                    [1,2,3],\n",
    "#                    [7,5,3],\n",
    "#                    [5,3,6]]], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "print(t.shape)\n",
    "print(t)\n",
    "\n",
    "t2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "# t2 = nn.AvgPool2d(kernel_size=2, stride=1, padding=1)\n",
    "\n",
    "print()\n",
    "# print(t2(t).shape)\n",
    "# print(t2(t))\n",
    "\n",
    "t3 = torch.flatten(t, 1)\n",
    "print()\n",
    "print(t3.shape)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar2KQtFKC0yJ"
   },
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers + one fully connected layer)\n",
    "\"\"\"nn.Conv2d(in_channels, out_channels, kernel_size)\"\"\"\n",
    "\"\"\"nn.MaxPool2d(kernel_size, stride)\"\"\"\n",
    "\"\"\"nn.Linear(in_features, out_features)\"\"\"\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        # super(ConvNet, self).__init__()\n",
    "        super().__init__()\n",
    "        #First convolutional layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #Second convolutional layer\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #Fully connected layer with 2 hidden layers + output. Output has 10 neurons, 1 for each class\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x) \n",
    "        out = self.layer2(out) \n",
    "        out = torch.flatten(out, 1) # flatten all dimensions except batch\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "\n",
    "convnet = ConvNet(10).to(device) #set up for GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCGIWAj5xiiB"
   },
   "source": [
    "## Optimizers - [source](https://medium.com/@Biboswan98/optim-adam-vs-optim-sgd-lets-dive-in-8dbf1890fbdc)\n",
    "* **S**tochastic **G**radient **D**escent \n",
    "* **ADA**ptive **M**oment optimizer \\\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIGmpKBrnGTo"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(convnet.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(convnet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGmA78mvLSub"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8BGoGnALUWf",
    "outputId": "4b3832bb-f754-4b9e-808b-58a74d47bcb8"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "for epoch in range(NUM_EPOCHS):  # loop over the dataset multiple times\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #inputs, labels = data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device) #set for GPU if available\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = convnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() # updating the weights of the network\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'Iteration {i+1}, loss = {(running_loss / 2000):.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_UGvOftOdoI"
   },
   "source": [
    "#### Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjK3mmTmnV5z"
   },
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(convnet.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWjGh5mEOgA4"
   },
   "source": [
    "#### Load saved model and predict some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "hGoYrnf-njW0",
    "outputId": "c8d892e0-79e6-47ee-9f64-08becf5673bd"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "data = next(dataiter)\n",
    "images = data[0]\n",
    "labels = data[1]\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "#images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "convnet = ConvNet()#.to(device)\n",
    "convnet.load_state_dict(torch.load(PATH))\n",
    "\n",
    "outputs = convnet(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcFLmO7COuIP"
   },
   "source": [
    "#### Accuracy on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMiNPQ6Pnxiv",
    "outputId": "9f853bd0-beb7-482b-c43b-49691c0a9bfa"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        #inputs, labels = data[0].to(device), data[1].to(device) #set for GPU if available\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = convnet(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn6vLuQyOwu3"
   },
   "source": [
    "#### Accuracy on test set per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFIK-xaOn3dP",
    "outputId": "fd8cdaef-dccf-4b50-ac77-db50892883e0"
   },
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        #inputs, labels = data[0].to(device), data[1].to(device) #set for GPU if available\n",
    "        outputs = convnet(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn. Choose different dataset, modify the architecture and do experiment on it, then see the result."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Lab3 - Deep Network Developments - CNNs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03953809ceab4a368c3e98be687049e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0b70957757a7404bb0949fc2518f4eaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1132508146f543888f3358fb4b33bb52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be84f0e6beb34398b75184f26c30bac8",
      "placeholder": "​",
      "style": "IPY_MODEL_ee63ff85590d4f3b9ef1511daecb53eb",
      "value": ""
     }
    },
    "21de6de5723647b2b1c511f9c523d80b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1132508146f543888f3358fb4b33bb52",
       "IPY_MODEL_f3a19f4144e54caaaf51ced4046a3165",
       "IPY_MODEL_b7a93f3117914831ac1a929c622ae529"
      ],
      "layout": "IPY_MODEL_aab2327d02024e1fbc22178eda86f049"
     }
    },
    "5a1caefed15a4f83b0728604c7ed8c02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aab2327d02024e1fbc22178eda86f049": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7a93f3117914831ac1a929c622ae529": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de2e53173a5543cf9b74e03ec58ca279",
      "placeholder": "​",
      "style": "IPY_MODEL_5a1caefed15a4f83b0728604c7ed8c02",
      "value": " 170499072/? [00:03&lt;00:00, 50517602.46it/s]"
     }
    },
    "be84f0e6beb34398b75184f26c30bac8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de2e53173a5543cf9b74e03ec58ca279": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee63ff85590d4f3b9ef1511daecb53eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3a19f4144e54caaaf51ced4046a3165": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b70957757a7404bb0949fc2518f4eaf",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_03953809ceab4a368c3e98be687049e1",
      "value": 170498071
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
